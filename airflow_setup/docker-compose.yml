services:
  kafka-broker:
    image: apache/kafka:4.0.0
    container_name: kafka-broker
    ports:
      - "9092:9092"
    networks:
      - local-iceberg-lakehouse
    environment:
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: 'broker,controller'
      KAFKA_LISTENERS: 'PLAINTEXT://kafka-broker:29092,CONTROLLER://kafka-broker:29093,PLAINTEXT_HOST://0.0.0.0:9092'
      KAFKA_ADVERTISED_LISTENERS: 'PLAINTEXT://kafka-broker:29092,PLAINTEXT_HOST://localhost:9092'
      KAFKA_CONTROLLER_LISTENER_NAMES: 'CONTROLLER'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT'
      KAFKA_CONTROLLER_QUORUM_VOTERS: '1@kafka-broker:29093'
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1

  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    ports:
      - "9090:8080"
    networks:
      - local-iceberg-lakehouse
    depends_on:
      - kafka-broker
    environment:
      KAFKA_CLUSTERS_0_NAME: local-iceberg-cluster
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka-broker:29092

  trino:
    image: trinodb/trino:latest
    container_name: trino
    ports:
      - "8082:8080"
    environment:
      - TRINO_JVM_OPTS=-Xmx2G
    networks:
      - local-iceberg-lakehouse
    volumes:
      - ./trino/catalog:/etc/trino/catalog

  polaris:
    image: apache/polaris:latest
    container_name: polaris
    ports:
      - "8181:8181"
      - "8182:8182"
    networks:
      - local-iceberg-lakehouse
    depends_on:
      - minio
    environment:
      AWS_ACCESS_KEY_ID: admin
      AWS_SECRET_ACCESS_KEY: password
      AWS_REGION: us-east-1
      AWS_ENDPOINT_URL_S3: http://minio:9000
      AWS_ENDPOINT_URL_STS: http://minio:9000
      POLARIS_BOOTSTRAP_CREDENTIALS: default-realm,root,secret
      polaris.features.DROP_WITH_PURGE_ENABLED: "true"
      polaris.realm-context.realms: default-realm
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8182/healthcheck"]
      interval: 10s

  minio:
    image: minio/minio:latest
    container_name: minio
    ports:
      - "9001:9001"
      - "9000:9000"
    environment:
      AWS_ACCESS_KEY_ID: admin
      AWS_SECRET_ACCESS_KEY: password
      AWS_REGION: dummy-region
      MINIO_ROOT_USER: admin
      MINIO_ROOT_PASSWORD: password
      MINIO_DOMAIN: minio
    networks:
      local-iceberg-lakehouse:
        aliases:
          - warehouse.minio
    command: ["server", "/data", "--console-address", ":9001"]

  minio-client:
    image: minio/mc:latest
    depends_on:
      - minio
    networks:
      - local-iceberg-lakehouse
    volumes:
      - /tmp:/tmp
    environment:
      AWS_ACCESS_KEY_ID: admin
      AWS_SECRET_ACCESS_KEY: password
      AWS_REGION: us-east-1
    entrypoint: >
      /bin/sh -c "  
      until (mc alias set minio http://minio:9000 admin password) do echo '...waiting...' && sleep 1; done;  
      mc rm -r --force minio/warehouse;  
      mc mb minio/warehouse;  
      mc anonymous set public minio/warehouse;  
      tail -f /dev/null  
      "   

  superset:
    image: apache/superset:latest
    container_name: superset-1
    user: "root"
    ports:
      - "8088:8088"
    environment:
      - SUPERSET_SECRET_KEY=admin
      - PYTHONPATH=/app/pythonpath
      - SUPERSET_FEATURE_FLAGS={"ENABLE_TEMPLATE_PROCESSING":true}
      - SESSION_COOKIE_NAME=superset_session
    command: >
      bash -c "pip install --target /app/.venv/lib/python3.10/site-packages trino sqlalchemy-trino &&
               superset db upgrade &&
               superset fab create-admin --username admin --firstname Manish --lastname Tewari --email admin@admin.com --password admin || true &&
               superset init &&
               gunicorn --bind 0.0.0.0:8088 --workers 1 --timeout 120 --limit-request-line 0 --limit-request-field_size 0 'superset.app:create_app()'"
    volumes:
      - ./superset_home:/app/superset_home
      - /mnt/e/docker/airflow/superset_config:/app/pythonpath
    networks:
      - local-iceberg-lakehouse

  spark-master:
    image: apache/spark:3.5.2
    container_name: spark-master
    user: root
    environment:
      - SPARK_MODE=master
      - SPARK_DAEMON_CLASSPATH=/opt/airflow/lib/*
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
    ports:
      - "8080:8080"
      - "7077:7077"
    volumes:
      - /mnt/e/docker/airflow/lib:/opt/airflow/lib
      - spark_ivy_cache:/tmp/.ivy2
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
    networks:
      - local-iceberg-lakehouse

  spark-worker:
    image: apache/spark:3.5.2
    container_name: spark-worker
    user: root
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_CLASSPATH=/opt/airflow/lib/*
      - SPARK_WORKER_MEMORY=4G
      - SPARK_WORKER_CORES=2
    volumes:
      - /mnt/e/docker/airflow/lib:/opt/airflow/lib
      - spark_ivy_cache:/tmp/.ivy2'
    mem_limit: 4G
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    networks:
      - local-iceberg-lakehouse

  airflow-db:
    image: postgres:13
    container_name: airflow-db
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - local-iceberg-lakehouse

  airflow:
    build: .
    container_name: docker-airflow-1
    depends_on:
      airflow-db: { condition: service_healthy }
      spark-master: { condition: service_started }
    environment:
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@airflow-db/airflow
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW_HOME=/opt/airflow
      - _AIRFLOW_WWW_USER_USERNAME=admin
      - _AIRFLOW_WWW_USER_PASSWORD=password
      - _AIRFLOW_WWW_USER_CREATE=true
      - AIRFLOW__WEBSERVER__COOKIE_NAME=airflow_session
      - SPARK_DRIVER_MEMORY=2G
    ports: ["8085:8080"]
    mem_limit: 4G
    volumes:
      - /mnt/e/docker/airflow/dags:/opt/airflow/dags
      - /mnt/e/docker/airflow/logs:/opt/airflow/logs
      - /mnt/e/docker/airflow/plugins:/opt/airflow/plugins
      - /mnt/e/docker/airflow/lib:/opt/airflow/lib

    networks:
      local-iceberg-lakehouse:
        aliases:
          - airflow-driver
    command: >
      bash -c "
      /opt/airflow/download_jars.sh &&
      airflow db migrate &&
      airflow users create --username admin --password password --firstname Admin --lastname User --role Admin --email admin@example.com || true &&
      airflow users reset-password --username admin --password password &&
      airflow connections add 'spark_default' --conn-type 'spark' --conn-host 'spark://spark-master' --conn-port '7077' || true &&
      (airflow scheduler & airflow webserver)
      "

networks:
  local-iceberg-lakehouse:
    name: local-iceberg-lakehouse
    driver: bridge
volumes:
  spark_ivy_cache: